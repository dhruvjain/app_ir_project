Question Terminology and Representation for
Question Type Classication
Noriko Tomuro
DePaul University
School of Computer Science Telecommunications and Information Systems
 S Wabash Ave
Chicago IL  USA
tomurocsdepauledu
Abstract
Question terminology is a set of terms which ap
pear in keywords idioms and xed expressions
commonly observed in questions This paper
investigates ways to automatically extract ques
tion terminology from a corpus of questions and
represent them for the purpose of classifying by
question type Our key interest is to see whether
or not semantic features can enhance the repre
sentation of strongly lexical nature of question
sentences We compare two feature sets one
with lexical features only and another with a
mixture of lexical and semantic features For
evaluation we measure the classication accu
racy made by two machine learning algorithms
C and PEBLS by using a procedure called
domain crossvalidation which eectively mea
sures the domain transferability of features
 Introduction
In Information Retrieval 	IR
 text categoriza
tion and clustering documents are usually in
dexed and represented by domain terminology
terms which are particular to the domaintopic
of a document However when documents must
be retrieved or categorized according to criteria
which do not correspond to the domains such as
genre 	text style
 	Kessler et al  Finn et
al 
 or subjectivity 	eg opinion vs fac
tual description
 	Wiebe 
 we must use
dierent domainindependent features to index
and represent documents In those tasks selec
tion of the features is in fact one of the most
critical factors which aect the performance of
a system
Question type classication is one of such
tasks where categories are question types 	eg
howto why and where
 In recent years
question type has been successfully used in
many QuestionAnswering 	QA
 systems for
determining the kind of entity or concept be
ing asked and extracting an appropriate answer
	Voorhees  Harabagiu et al  Hovy
et al 
 Just like genre question types
cut across domains for instance we can ask
howto questions in the cooking domain the
legal domain etc However features that consti
tute question types are dierent from those used
for genre classication 	typically partofspeech
or metalingusitic features
 in that features are
strongly lexical due to the large amount of id
iosyncrasy 	keywords idioms or syntactic con
structions
 that is frequently observed in ques
tion sentences For example we can easily think
of question patterns such as What is the best
way to  and What do I have to do to  In
this regard terms which identify question type
are considered to form a terminology of their
own which we dene as question terminology
Terms in question terminology have some
characteristics First they are mostly domain
independent noncontent words Second they
include many closedclass words 	such as in
terrogatives modals and pronouns
 and some
openclass words 	eg the noun way and the
verb do
 In a way question terminology is a
complement of domain terminology
Automatic extraction of question terminology
is a rather dicult task since question terms are
mixed in with content terms Another compli
cating factor is paraphrasing  there are many
ways to ask the same question For example
 How can I clean teapots
 In what way can we clean teapots
 What is the best way to clean teapots
 What method is used for cleaning teapots
 How do I go about cleaning teapots
In this paper we present the results of our
investigation on how to automatically extract
question terminology from a corpus of questions
and represent them for the purpose of classi
fying by question type It is an extension of
our previous work 	Tomuro and Lytinen 

where we compared automatic and manual tech
niques to select features from questions but
only 	stemmed
 words were considered for fea
tures The focus of the current work is to in
vestigate the kinds of features rather than
selection techniques which are best suited for
representing questions for classication Specif
ically from a large dataset of questions we au
tomatically extracted two sets of features one
set consisting of terms 	ie lexical features

only and another set consisting of a mixture of
terms and semantic concepts 	ie semantic fea
tures
 Our particular interest is to see whether
or not semantic concepts can enhance the repre
sentation of strongly lexical nature of question
sentences To this end we apply two machine
learning algorithms 	C 	Quinlan 
 and
PEBLS 	Cost and Salzberg 

 and com
pare the classication accuracy produced for the
two feature sets The results show that there is
no signicant increase by either algorithm by
the addition of semantic features
The original motivation behind our work on
question terminology was to improve the re
trieval accuracy of our system called FAQFinder
	Burke et al  Lytinen and Tomuro 

FAQFinder is a webbased natural language
QA system which uses Usenet Frequently
Asked Questions 	FAQ
 les to answer users
questions Figures  and  show an example ses
sion with FAQFinder First the user enters a
question in natural language The system then
searches the FAQ les for questions that are
similar to the users Based on the results of
the search FAQFinder displays a maximum of
 FAQ questions which are ranked the highest
by the systems similarity measure Currently
FAQFinder incorporates question type as one of
the four metrics in measuring the similarity be
tween the users question and FAQ questions

In the present implementation the system uses
a small set of manually selected words to deter
mine the type of a question The goal of our
work here is to derive optimal features which
would produce improved classication accuracy

The other three metrics are vector similarity seman
tic similarity and coverage Lytinen and Tomuro 
Figure  User question entered as a natural
language query to FAQFinder
Figure  The  bestmatching FAQ questions
 Question Types
In our work we dened  question types below
 DEF definition  PRC procedure
 REF reference  MNR manner
 TME time  DEG degree
 LOC location 	 ATR atrans

 ENT entity  INT interval
 RSN reason  YNQ yesno
Descriptive denitions of these types are
found in 	Tomuro and Lytinen 
 Table
 shows example FAQ questions which we had
used to develop the question types Note that
our question types are general question cate
gories They are aimed to cover a wide variety
of questions entered by the FAQFinder users
 Selection of Feature Sets
In our current work we utilized two feature sets
one set consisting of lexical features only 	LEX

and another set consisting of a mixture of lexi
cal features and semantic concepts 	LEXSEM

Obviously there are many known keywords id
ioms and xed expressions commonly observed
in question sentences However categorization
of some of our  question types seem to de
pend on openclass words for instance What
does mpg mean 	DEF
 and What does Bel
gium import and export 	REF
 To distin
guish those types semantic features seem eec
tive Semantic features could also be useful as
backo features since they allow for generaliza
tion For example in WordNet 	Miller 

the noun knowhow is encoded as a hypernym
of method methodology solution and
technique By selecting such abstract con
cepts as semantic features we can cover a va
riety of paraphrases even for xed expressions
and supplement the coverage of lexical features
We selected the two feature sets in the follow
ing two steps In the rst step using a dataset
of  example questions taken from  FAQ
lesdomains we rst manually tagged each
question by question type and then automat
ically derived the initial lexical set and initial
semantic set Then in the second step we re
ned those initial sets by pruning irrelevant fea
tures and derived two subsets LEX from the
initial lexical set and LEXSEM from the union
of lexical and semantic sets
To evaluate various subsets tried during
the selection steps we applied two machine
learning algorithms C 	the commercial
version of C 	Quinlan 
 available
at httpwwwrulequestcom
 a decision tree
classier and PEBLS 	Cost and Salzberg

 a knearest neighbor algorithm

We
also measured the classication accuracy by
a procedure we call domain crossvalidation
	DCV
 DCV is a variation of the standard
crossvalidation 	CV
 where the data is parti
tioned according to domains instead of random

We used k  	 and majority voting scheme for all
experiments in our current work
choice To do a kfold DCV on a set of ex
amples from n domains the set is rst broken
into k nonoverlapping blocks where each block
contains examples exactly from m 
n
k
do
mains Then in each fold a classier is trained
with 	k  
  m domains and tested on ex
amples from m unseen domains Thus by ob
serving the classication accuracy of the target
categories using DCV we can measure the do
main transferability how well the features ex
tracted from some domains transfer to other do
mains Since question terminology is essentially
domainindependent DCV is a better evalua
tion measure than CV for our purpose
 Initial Lexical Set
The initial lexical set was obtained by ordering
the words in the dataset by their Gain Ratio
scores then selecting the subset which produced
the best classication accuracy by C and PE
BLS Gain Ratio 	GR
 is a metric often used
in classication systems 	notably in C
 for
measuring how well a feature predicts the cate
gories of the examples GR is a normalized ver
sion of another metric called Information Gain
	IG
 which measures the informativeness of a
feature by the number of bits required to en
code the examples if they are partitioned into
two sets based on the presence or absence of
the feature

Let C denote the set of categories c

  c
m
for which the examples are classied 	ie tar
get categories
 Given a collection of examples
S the Gain Ratio of a feature A GR	SA
 is
dened as
GR	SA
 
IG	SA

SI	SA

where IG	SA
 is the Information Gain dened
to be
IGSA  
P
m
i
Prc
i
 log

Prc
i

PrA
P
m
i
Prc
i
jA log

Prc
i
jA
PrA
P
m
i
Prc
i
jA log

Prc
i
jA
and SI	SA
 is the Splitting Information de
ned to be
SISA  PrA log

PrA  PrA log

PrA

The description of Information Gain here is for bi
nary partitioning Information Gain can also be gener
alized to mway partitioning for all m  
Table  Example FAQ questions
Question Type Question
DEF What does reactivity of emissions mean
REF What do mutual funds invest in
TME What dates are important when investing in mutual funds
ENT Who invented Octane Ratings
RSN Why does the Moon always show the same face to the Earth
PRC How can I get rid of a caeine habit
MNR How did the solar system form
ATR Where can I get British tea in the United States
INT When will the sun die
YNQ Is the Moon moving away from the Earth
Then features which yield high GR values are
good predictors In previous work in text cat
egorization GR 	or IG
 has been shown to be
one of the most eective methods for reducing
dimensions 	ie words to represent each text

	Yang and Pedersen 

Here in applying GR there was one issue
we had to consider how to distinguish con
tent words from noncontent words This issue
arose from the uneven distribution of the ques
tion types in the dataset Since not all question
types were represented in every domain if we
chose question type as the target category fea
tures which yield high GR values might include
some domainspecic words In eect good pre
dictors for our purpose are words which predict
question types very well but do not predict do
mains Therefore we dened the GR score of a
word to be the combination of two values the
GR value when the target category was ques
tion type minus the GR value when the target
category was domain
We computed the 	modied
 GR score for
 words which appeared more than twice in
the dataset and applied C and PEBLS Then
we gradually reduced the set by taking the top n
words according to the GR scores and observed
changes in the classication accuracy Figure 
shows the result The evaluation was done by
using the fold DCV and the accuracy percent
ages indicated in the gure were an average of
 runs The best accuracy was achieved by the
top  words by both algorithms the remain
ing words seemed to have caused overtting as
the accuracy showed slight decline Thus we
took the top  words as the initial lexical fea
ture set
0
10
20
30
40
50
60
70
80
90
0 200 400 600 800 1000 1200 1400
# features
A
cc
ur
ac
y 
(%
)
C5.0
PEBLS
Figure  Classication Accuracy 	
 on the
training data measured by Domain Cross Vali
dation 	DCV

 Initial Semantic Set
The initial semantic set was obtained by au
tomatically selecting some nodes in the Word
Net 	Miller 
 noun and verb trees For
each question type we chose questions of cer
tain structures and applied a shallow parser to
extract nouns andor verbs which appeared at
a specic position For example for all ques
tion types 	except for YNQ
 we extracted the
head noun from questions of the form What
is NP  Those nouns are essentially the
denominalization of the question type The
nouns extracted included way method
procedure process for the type PRC rea
son advantage for RSN and organization
restaurant for ENT For the types DEF and
MNR we also extracted the main verb from
questions of the form HowWhat does NP V
 Such verbs included work mean for
DEF and aect and form for MNR
Then for the nouns and verbs extracted for
each question type we applied the sense dis
ambiguation algorithm used in 	Resnik 

and derived semantic classes 	or nodes in the
WordNet trees
 which were their abstract gen
eralization For each word in a set we traversed
the WordNet tree upward through the hyper
nym links from the nodes which corresponded
to the rst two senses of the word and assigned
each ancestor a value which equaled to the in
verse of the distance 	ie the number of links
traversed
 from the original node Then we
accumulated the values for all ancestors and
selected ones 	excluding the top nodes
 whose
value was above a threshold For example
the set of nouns extracted for the type PRC
were knowhow 	an ancestor of way and
method
 and activity 	an ancestor of pro
cedure and process

By applying the procedure above for all ques
tion types we obtained a total of  semantic
classes This constitutes the initial semantic set
 Renement
The nal feature sets LEX and LEXSEM were
derived by further rening the initial sets The
main purpose of renement was to reduce the
union of initial lexical and semantic sets 	a to
tal of      features
 and derive
LEXSEM It was done by taking the features
which appeared in more than half of the deci
sion trees induced by C during the iterations
of DCV

Then we applied the same procedure
to the initial lexical set 	 features
 and de
rived LEX Now both sets were 	sub
 optimal
subsets with which we could make a fair com
parison There were  featureswords and 
features selected for LEX and LEXSEM respec
tively
Our renement method is similar to 	Cardie

 in that it selects features by removing
ones that did not appear in a decision tree
The dierence is that in our method each de
cision tree is induced from a strict subset of
the domains of the dataset Therefore by tak
ing the intersection of multiple such trees we
can eectively extract features that are domain
independent thus transferable to other unseen
domains Our method is also computationally

We have in fact experimented various threshold val
ues It turned out that 
 produced the best accuracy
Table  Classication accuracy 	
 on the
training set by using reduced feature sets
Feature set  features C
	 PEBLS
Initial lex 
	  
LEX reduced   

Initial lex  sem   
LEXSEM reduced   
less expensive and feasible given the number of
features expected to be in the reduced set 	over
a hundred by our intuition
 than other fea
ture subset selection techniques most of which
require expensive search through model space
	such as wrapper approach 	John et al 


Table  shows the classication accuracy mea
sured by DCV for the training set The increase
of the accuracy after the renement was mini
mal using C 	from  to  for LEX from
 to  for LEXSEM
 as expected But
the increase using PEBLS was rather signicant
	from  to  for LEX from  to  for
LEXSEM
 This result agreed with the ndings
in 	Cardie 
 and conrmed that LEX and
LEXSEM were indeed 	sub
 optimal However
the dierence between LEX and LEXSEM was
not statistically signicant by either algorithm
	from  to  by C from  to 
by PEBLS pvalues were  and  respec
tively


 This means the semantic features did
not help improve the classication accuracy
As we inspected the results we discovered
that out of the  features in LEXSEM 
were semantic features and they did occur in
 of the training examples 	 

 However in most of those examples key
terms were already represented by lexical fea
tures thus semantic features did not add any
more information to help determine the ques
tion type As an example a sentence What
are the dates of the upcoming Jewish holi
days was represented by lexical features
what be of and date and a seman
tic feature timeunit 	an ancestor of date

The  words in LEX are listed in the Ap
pendix at the end of this paper

Pvalues were obtained by applying the ttest on
the accuracy produced by all iterations of DCV with
a null hypothesis that the mean accuracy of LEXSEM
was higher than that of LEX
Table  Classication accuracy 	
 on the testsets
Feature set  FAQFinder AskJeeves
features C PEBLS C PEBLS
LEX     
LEXSEM     
 External Testsets
To further investigate the eect of semantic fea
tures we tested LEX and LEXSEM with two
external testsets one set consisting of  ques
tions taken from FAQFinder user log and an
other set consisting of  questions taken from
the AskJeeves 	httpwwwaskjeevescom
 user
log Both datasets contained questions from a
wide range of domains therefore served as an
excellent indicator of the domain transferability
for our two feature sets
Table  shows the results For the FAQFinder
data LEX and LEXSEM produced compara
ble accuracy using both C and PEBLS But
for the AskJeeves data LEXSEM did worse
than LEX consistently by both classiers This
means the additional semantic features were in
teracting with lexical features
We speculate the reason to be the follow
ing Compared to the FAQFinder data the
AskJeeves data was gathered from a much wider
audience and the questions spanned a broad
range of domains Many terms in the questions
were from vocabulary considerably larger than
that of our training set Therefore the data
contained quite a few words whose hypernym
links lead to a semantic feature in LEXSEM
but did not fall into the question type keyed
by the feature For instance a question in
AskJeeves What does Hanukah mean was
misclassied as type TME by using LEXSEM
This was because Hanukah in WordNet was
encoded as a hyponym of time period On the
other hand LEX did not include Hanukah
thus correctly classied the question as type
DEF
 Related Work
Recently with a need to incorporate user prefer
ences in information retrieval several work has
been done which classies documents by genre
For instance 	Finn et al 
 used machine
learning techniques to identify subjective 	opin
ion
 documents from newspaper articles To de
termine what feature adapts well to unseen do
mains they compared three kinds of features
words partofspeech statistics and manually
selected metalinguistic features They con
cluded that the partofspeech performed the
best with regard to domain transfer However
not only were their feature sets predetermined
their features were distinct from words in the
documents 	or features were the entire words
themselves
 thus no feature subset selection
was performed
	Wiebe 
 also used machine learning
techniques to identify subjective sentences She
focused on adjectives as an indicator of sub
jectivity and used corpus statistics and lexical
semantic information to derive adjectives that
yielded high precision
 Conclusions and Future Work
In this paper we showed that semantic features
did not enhance lexical features in the represen
tation of questions for the purpose of question
type classication While semantic features al
low for generalization they also seemed to do
more harm than good in some cases by inter
acting with lexical features This indicates that
question terminology is strongly lexical indeed
and suggests that enumeration of words which
appear in typical idiomatic question phrases
would be more eective than semantics
For future work we are planning to exper
iment with synonyms The use of synonyms
is another way of increasing the coverage of
question terminology while semantic features
try to achieve it by generalization synonyms
do it by lexical expansion Our plan is to use
the synonyms obtained from very large cor
pora reported in 	Lin 
 We are also
planning to compare the 	lexical and seman
tic
 features we derived automatically in this
work with manually selected features In our
previous work manually selected 	lexical
 fea
tures showed slightly better performance for the
training data but no signicant dierence for
the test data We plan to manually pick out se
mantic as well as lexical features and apply to
the current data
References
R Burke K Hammond V Kulyukin S Lyti
nen N Tomuro and S Schoenberg 
Question answering from frequently asked
question les Experiences with the faqnder
system AI Magazine 	

C Cardie  Using decision trees to im
prove casebased learning In Proceedings of
the th International Conference on Ma
chine Learning ICML
S Cost and S Salzberg  A weighted near
est neighbor algorithm for learning with sym
bolic features Machine Learning 	

A Finn N Kushmerick and B Smyth 
Genre classication and domain transfer for
information ltering In Proceedings of the
European Colloquium on Information Re
trieval Research Glasgow
S Harabagiu D Moldovan M Pasca R Mi
halcea M Surdeanu R Bunescu R Girju
V Rus and P Morarescu  Falcon
Boosting knowledge for answer engines In
Proceedings of TREC
E Hovy L Gerber U Hermjakob C Lin and
D Ravichandran  Toward semantics
based answer pinpointing In Proceedings of
the DARPA Human Language Technologies
HLT
G John R Kohavi and K P eger  Irrel
evant features and the subset selection prob
lem In Proceedings of the th International
Conference on Machine Learning ICML	
K Kessler G Nunberg and H Schutze 
Automatic detection of text genre In Pro
ceedings of the 
th Annual Meeting of the
Association for Computational Linguistics
ACL
D Lin  Automatic retrieval and cluster
ing of similar words In Proceedings of the
th Annual Meeting of the Association for
Computational Linguistics ACL
S Lytinen and N Tomuro  The use
of question types to match questions in
faqnder In Papers from the  AAAI
Spring Symposium on Mining Answers from
Texts and Knowledge Bases
G Miller  Wordnet An online lexical
database International Journal of Lexicog
raphy 	

R Quinlan  C	
 Programs for Machine
Learning Morgan Kaufman
P Resnik  Selectional preference and
sense disambiguation In Proceedings of the
ACL SIGLEX Workshop on Tagging Text
with Lexical Semantics Washington DC
N Tomuro and S Lytinen  Selecting
features for paraphrasing question sentences
In Proceedings of the workshop on Auto
matic Paraphrasing at NLP Pacic Rim 
NLPRS Tokyo Japan
E Voorhees  The trec question answer
ing track report In Proceedings of TREC
J Wiebe  Learning subjective adjectives
from corpora In Proceedings of the th Na
tional Conference on Articial Intelligence
AAAI Austin Texas
Y Yang and J Pedersen  A comparative
study on feature selection in text categoriza
tion In Proceedings of the 	th International
Conference on Machine Learning ICML
Appendix The LEX Set
about address advantage aect and
any archive available bag be begin
benet better buy can cause clean
come company compare contact conta
gious copy cost create date day deal
dier dierence do eect emission evap
orative expense fast nd for get go
good handle happen have history how
if in internet keep know learn long
make many mean milk much my
name number obtain of often old on
one or organization origin people per
centage place planet price procedure pro
nounce purpose reason relate relationship
shall shuttle site size sky so solar
some start store sun symptom take
tank tax that there time to us way
web what when where which who
why will with work world wide web
wrong www year you
